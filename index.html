<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models</title>
  <link rel="icon" type="image/x-icon" href="static/images/logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css" integrity="sha384-wcIxkf4k558AjM3Yz3BBFQUbk/zgIYC2R0QpeeYb+TwlBVMrlgLqwRjRtGZiK7ww" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js" integrity="sha384-hIoBPJpTUs74ddyc4bFZSM1TVlQDA60VBbJS0oA934VSz82sBx1X7kSx2ATBDIyd" crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js" integrity="sha384-43gviWU0YVjaDtb/GhzOouOXtZMP/7XUzwPTstBeZFe/+rCMvRwr4yROQP43s0Xk" crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement(document.body, {
          // customised options
          // • auto-render specific keys, e.g.:
          delimiters: [
              {left: '$$', right: '$$', display: true},
              {left: '$', right: '$', display: false},
              {left: '\\(', right: '\\)', display: false},
              {left: '\\[', right: '\\]', display: true}
          ],
          // • rendering keys, e.g.:
          throwOnError : false
        });
    });
</script>



  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
                <span class="author-block">
                  <a href="https://zhangce01.github.io/" target="_blank">Ce Zhang</a>,</span>
                  <span class="author-block">
                     <a href="https://simonstepputtis.com/" target="_blank">Simon Stepputtis</a>,</span>
                     <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=VWv6a9kAAAAJ&hl=en" target="_blank">Katia Sycara</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://yaqi-xie.me/" target="_blank">Yaqi Xie</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">School of Computer Science, Carnegie Mellon University<br><b>NeurIPS 2024</b></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">

                    <!-- PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/NeurIPS_2024_Test_Time.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Paper</span>
                    </a>
                  </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/1111.11111" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/zhangce01/DPE-CLIP" target="_blank"
                class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                  <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
              </span>
              <figure>
                <br>
                <img src="static/images/comparison.png" alt="fail" width="100%"">
                <figcaption class="content has-text-left"  style="word-break:normal"><b>Figure 1. Comparison of our DPE with zero-shot CLIP, TPT, and TDA.</b>
                  We denote CLIP's parallel textual and visual encoders as $\mathcal{E}_t$ and $\mathcal{E}_v$, respectively. While previous methods solely adapt the CLIP model from a single modality, we design our DPE to evolve prototypes from both textual and visual modalities to progressively capture more accurate multi-modal representations for target classes during test time.</figcaption>
              </figure>
              <figure>
                <br>
                <img src="static/images/overview2.png" alt="fail" width="100%"">
                <figcaption class="content has-text-left"  style="word-break:normal"><b>Figure 2. An overview of our DPE method.</b>
                  We introduce prototypes from both textual and visual modalities and enable prototype-based inference with CLIP. For each test sample, we optimize both prototypes using learnable residual parameters with alignment loss $\mathcal{L}_{\mathsf{align}}$ and self-entropy loss $\mathcal{L}_{\mathsf{aug}}$. These prototypes are also progressively evolved over time to capture more accurate and discriminative multi-modal representations for target classes.
                </figcaption>
              </figure>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper motivation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Highlights</h2>
        <div class="content has-text-justified">
          <p>
            <ul>
              <li>We propose dual prototype evolving (DPE), a novel test-time adaptation method for VLMs that <i>progressively</i> captures more accurate <i>multi-modal</i> representations for target classes during test time.</li>
              <li>To promote consistent multi-modal representations, we introduce and optimize learnable residuals for each test sample to align the prototypes across modalities.</li>
              <li>Experimental evaluations demonstrate that our DPE consistently outperforms current state-of-the-art methods across 15 diverse datasets while maintaining competitive computational efficiency.</li>
            </ul>
          </p>
        </div>
      </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper abstract -->
<section class="section hero is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Test-time adaptation, which enables models to generalize to diverse data with unlabeled test samples, holds significant value in real-world scenarios.
            Recently, researchers have applied this setting to advanced pre-trained vision-language models (VLMs), developing approaches such as test-time prompt tuning to further extend their practical applicability. However, these methods typically focus solely on adapting VLMs from a single modality and fail to accumulate task-specific knowledge as more samples are processed. To address this, we introduce Dual Prototype Evolving (DPE), a novel test-time adaptation approach for VLMs that effectively <i>accumulates</i> task-specific knowledge from <i>multi-modalities</i>. Specifically, we create and evolve two sets of prototypes—textual and visual—to progressively capture more accurate multi-modal representations for target classes during test time.
            Moreover, to promote consistent multi-modal representations, we introduce and optimize learnable residuals for each test sample to align the prototypes from both modalities.
            Extensive experimental results on 15 benchmark datasets demonstrate that our proposed DPE consistently outperforms previous state-of-the-art methods while also exhibiting competitive computational efficiency.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Paper motivation -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            <ul>
              <li><b>Background</b>: Current VLMs excel at general-purpose classifications, but often struggle in highly specialized domains due to distribution shift.</li>
              <li><b>Goal</b>: In this work, we propose a novel approach for test-time adaptation (TTA) that refines predictions using only unlabeled data.
              </li>
              <li><b>Current Limitations of SOTA for unlabeled data</b>: (1) Existing methods treat each test sample as <i>independent</i> and need to <i>restart from the original model</i> for each sample; (2) TTA might benefit from utilizing multiple modalities, yet it is often only utilizing a single modality.</li>
              <li><b>Our Approach</b>: We propose <i>Dual Prototype Evolving (DPE)</i>, a novel test-time VLM adaptation approach that effectively <i>accumulates</i> task-specific knowledge from <i>multi-modalities</i>.</li>
            </ul>
          </p>
        </div>
      </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experimental Results</h2>

        <div class="content has-text-left is-size-5">
        <strong>Results on Robustness to Natural Distribution Shifts</strong>
        </div>
        <figure>
          <img src="static/images/ood_results.png" alt="fail" width="100%"">
          <figcaption class="content has-text-left" style="word-break:normal"><b>Table 1. Performance comparisons on robustness to natural distribution shifts.</b> We present top-1 accuracy (%) results for all evaluated methods employing both ResNet-50 and ViT-B/16 visual backbones of CLIP. The best results are highlighted in <b>bold</b>.
        </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Results on Cross-Datasets Generalization</strong>
        </div>
        <figure>
          <img src="static/images/cd_results.png" alt="fail" width="100%">
          <figcaption class="content has-text-left" style="word-break:normal">
            <b>Table 2. Performance comparisons on cross-datesets generalization.</b> We also present top-1 accuracy (\%) for all methods on two backbones of CLIP. The best results are highlighted in <b>bold</b>.
          </figure>
        <br>
        <div class="content has-text-left is-size-5">
          <strong>Efficiency Comparison</strong>
        </div>
        <figure>
          <img src="static/images/efficiency.png" alt="fail" width="50%">
          <figcaption class="content has-text-left" style="word-break:normal">
            <b>Table 3. Efficiency comparison on ImageNet.</b>
            We report the testing time, the achieved accuracy, and the performance gains compared to zero-shot CLIP.
          </figure>
          <br>
        <div class="content has-text-left is-size-5">
          <strong>Ablation Studies</strong>
        </div>
        <figure>
          <img src="static/images/ablation.png" alt="fail" width="100%">
          <figcaption class="content has-text-left" style="word-break:normal">
            <b>Figure 3. Ablation Studies.</b>
            (<i>Left</i>) Sensitivity analysis of $\tau_t$ and $M$ on Caltech101; (<i>Middle</i>) Analysis of the performance contributions from various learnable parameter settings across three datasets; (<i>Right</i>) Performance on three datasets with varying scale factor $\lambda$.
          </figure>
      </div>
    </div>
  </div>
</section>


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered" >
      <div class="column is-four-fifths" style="width:100%">
        <h2 class="title is 5">Video Demonstration</h2>
        <div class="content has-text-centered" style="word-break:normal" >
          <p>
            Video demonstration coming soon.</p>
        </div>
      </div>
    </div>
  </div>
</section>



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{zhang2024dual,
  title={Dual Prototype Evolving for Test-Time Generalization of Vision-Language Models},
  author={Zhang, Ce and Stepputtis, Simon and Sycara, Katia and Xie, Yaqi},
  journal={arXiv preprint arXiv:2403.12964},
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
